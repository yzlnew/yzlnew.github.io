---
title: 美年健康 AI 大赛初赛小结
date: 2018-05-07 22:01:51
description: 第一次，也是最后一次？
tags:
 - 机器学习
---

天池美年的初赛逐渐告一段落，作为一个之前几乎没有任何实际机器实践经验的选手，也勉强进了复赛（b 榜的分 0.0391 ）。在其中也学到了很多东西，有些部分留到后面有时间再总结，这里主要大致梳理一下初赛的心路历程。

![药丸][pills]

[pills]:https://1e8f2fa189da333464c9-e7cfef2b81d170385252e69507fe4f83.ssl.cf1.rackcdn.com/assets/images/pills.jpg
<!--more-->

### 参赛之前的背景

#### 为什么要参加这次比赛

非常直接的原因是找实习的时候发现必须得有项目经历，然而专业根本和机器学习没关系。在 kaggle 上看了几个入门赛，但是不如实际比赛来得有驱动力，再加上刚好美年的比赛时间很合适。

#### 比赛之前的技能点

- 自认为不太好的编程能力，会点 python，写过 C++ 算法，甚至还捣鼓过几个 Android 应用，~~学而不精的典范~~。
- 学过 Coursera 的 ML 课，看过《统计学习方法》，~~一问细节就懵逼~~。
- 良好的英文检索能力（我觉得这点还是挺重要的，知道点术语和自己想要实现的效果，剩下的谷歌能教会我就完全 OJBK）。
- 只听说过 XGBoost，完全没听说过 LightGBM，不过还是知道这样的比赛大多数都用的 GBDT 类的模型。Pandas？没听说过，numpy 都不 6。sklearn 还是用过一丢丢的（后来发现把它的文档整个通读一遍就知道基本的套路了，简直 excited）

然后就开始愉快地写代码了！不试试怎么知道自己不行呢！

### 比赛历程和一些坑

#### 开始上手

对于一个像我一样的初学者来说，面对一个题目一定是一脸茫然不知所措的。面对这种情况，两种方法是不推荐的：

1. 来吧，把机器学习 MOOC 从头来一遍。
2. 以小白的虚心姿态问群里的大佬。

第一种是因为做比赛其实一开始并不需要很全面的知识，像这个比赛，随便找个 kaggle 上类似比赛第一的解决方案看就行了。第二种，虽然比赛有很好的交流的机会，但是自己没有产出的话还是不要麻烦别人比较好，毕竟不是一个层次内的交流……

具体到这个比赛，我先看了几个 kaggle Titanic 的 [kernels](https://www.kaggle.com/c/titanic#tutorials)。有耐心的可以读一读[这个](https://www.kaggle.com/startupsci/titanic-data-science-solutions)，一整个 workflow 给你整得明明白白的。

另外调研了一下 LightGBM，感觉大家用得挺多的，那么我用它也肯定不这么会错咯。恰好 LightGBM 给了几个不错的 [Examples](https://github.com/Microsoft/LightGBM/tree/master/examples)，上手写代码的难度也大大降低了。

#### 先写一个简单的模型

第二步其实直接可以写了，根本没想什么特征提取啊、模型选择、参数优化（因为我都不会），所以就先拿 sklearn 写了一个随机森林的方法。这里要再赞美一下 sklearn 的文档和 API 写得真的好（主要还是因为它简单）。

用了 100 多个缺失值少于 50000 的数值特征（文字特征不会处理，到现在还是人工分类，没用 NLP，以及这里是个大坑），大概最后结果在 0.037 左右。虽然成绩完全不行，但是还是给了我很大的信心，起码我是可以做出结果来的。

#### 漫漫上分路

由于数据集的数据类型很复杂，数值列也基本不知道含义，取值范围也很多样，所以一开始就决定用树模型来做。特征决定了得分的上限，所以大部分时间基本都花在了处理数据、得到新的特征上去。后来证明这样是可行的。

1. 0.0320+ 阶段：只用 100 多个数值特征，LightGBM 用文档推荐的参数，大概得分在这里。
2. 0.0315 左右阶段：用 log1p 调整了训练集输出的分布。这个是第一个卡住的地方。算法也是比较不稳定（特征太少），产生测试集和验证集的 random_state 不同，结果变化挺大，但是线上的成绩是不会提高的。
3. 0.0310 阶段：手头还有好多的文字特征没用，取了几个数据量大的列，正则匹配分类，加了 5、6 个分类特征。虽然还是不稳定，但是线上成绩还是明显提高了（这又是卡住的地方）。
4. 0.0300+ 阶段：看了技术圈大佬的分享，发现自己的数值特征用少了，就放宽了缺失值的范围，可以说是大步飞跃，线上线下的成绩稳定、也很同步。之前估计进复赛起码得 0.0300 吧，所以信心还是增强了不少。
5. 0.0290+ 阶段：这时候离换数据只剩几天了，所以基本上就和队友在加文字特征，还是正则匹配分类。期间咨询了学医的同学的意见，选了一些重要的文字特征先处理了，效果还是很明显的。

其中还是遇到一些其他的坎坷，比如：

1. 队友用 LDA 处理了长的问题特征，线下成绩突飞猛进，线上成绩反而差了。最后得出的结论还是特征加的有问题，对于 NLP 的理解和参数选择都还有要学习的地方。
2. 研究了好久数据集要不要 shuffle 的问题，其实是毫无影响的。

### 一些感受和复赛的准备

总体来说地话，学了很多东西，还需要学的东西更多。在过程中能够体会几点：

1. 多参考别人的经验。自己犯的一些错误或者存在的误区，其实很多人都经历过。学习别人的经验成长是最快的。
2. 越往上，提升就越困难了，保持学习的心态，继续努力。

对于复赛，自己的理解，主要要发力还是处理文字特征。不过我始终认为 NLP 在这个问题上称作一个工具更合适，能帮助非专业的我们处理复杂的病情描述，实际效果怎么样，还是咨询一下专业人士比较好。另外一部分需要准备的就是模型调参和模型融合了，这个倒比前面的特征处理要稍微简单、有方向一点。

总之，继续加油吧！
